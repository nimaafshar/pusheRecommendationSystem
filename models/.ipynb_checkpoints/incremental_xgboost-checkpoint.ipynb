{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/root/data/deca/'\n",
    "DATA_PATH = '/Users/vahid/data/recommender/' \n",
    "CATEGORY = 2\n",
    "TRAIN_BATCH_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --proxy-user=iruser679343 --proxy-password=681811 -e use_proxy=yes -e http_proxy=us.mybestport.com:443 \n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/icons.csv.gz'-v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/notifs.txt.gz'-v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/random_submission.txt.gz'-v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/test.csv.gz' -v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/train_interactions.csv.gz' -v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/user_sparse_1.csv.gz'-v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/user_sparse_2.csv.gz'-v -P '/root/data/deca/'\n",
    "!wget 'http://hss.rahnema.ir/ml-data-2019/deca/users.csv.gz'-v -P '/root/data/deca/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "varss = sorted([(x, sys.getsizeof(globals().get(x)) // 1024**2) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n",
    "varss = [x for x in varss if x[1] > 0]\n",
    "varss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split as ttsplit\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "import pickle as pkl\n",
    "import os.path as path\n",
    "import os as os\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_boston()['data']\n",
    "y = load_boston()['target']\n",
    "\n",
    "# split data into training and testing sets\n",
    "# then split training set in half\n",
    "X_train, X_test, y_train, y_test = ttsplit(X, y, test_size=0.1, random_state=0)\n",
    "X_train_1, X_train_2, y_train_1, y_train_2 = ttsplit(X_train, \n",
    "                                                     y_train, \n",
    "                                                     test_size=0.5,\n",
    "                                                     random_state=0)\n",
    "xg_train_1 = xgb.DMatrix(X_train_1, label=y_train_1)\n",
    "xg_train_2 = xgb.DMatrix(X_train_2, label=y_train_2)\n",
    "xg_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {'objective': 'reg:linear', 'verbose': False}\n",
    "model_1 = xgb.train(params, xg_train_1, 30)\n",
    "model_1.save_model('model_1.model')\n",
    "\n",
    "# ================= train two versions of the model =====================#\n",
    "model_2_v1 = xgb.train(params, xg_train_2, 30)\n",
    "model_2_v2 = xgb.train(params, xg_train_2, 30, xgb_model='model_1.model')\n",
    "\n",
    "print(mse(model_1.predict(xg_test), y_test))     # benchmark\n",
    "print(mse(model_2_v1.predict(xg_test), y_test))  # \"before\"\n",
    "print(mse(model_2_v2.predict(xg_test), y_test))  # \"after\"\n",
    "\n",
    "# 23.0475232194\n",
    "# 39.6776876084\n",
    "# 27.2053239482\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH + 'train_interactions.csv')\n",
    "# tain_df = pd.read_csv(DATA_PATH + 'train_interactions_small.csv')\n",
    "train_df.drop(columns=['interaction_min', 'delivery_min','interaction_dow','interaction_hour','delivery_dow','delivery_hour'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(DATA_PATH + 'train_interactions_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'notif_id'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv(DATA_PATH + 'test.csv.gz')\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_prob = {}\n",
    "category_prob[0] = 0.089\n",
    "category_prob[1] = 0.078\n",
    "category_prob[2] = 0.059\n",
    "category_prob[3] = 0.049\n",
    "category_prob[4] = 0.041\n",
    "category_prob[5] = 0.053\n",
    "category_prob[7] = 0.058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982\n"
     ]
    }
   ],
   "source": [
    "notif_df = pd.read_csv('../data/notifs_corrected.csv')\n",
    "notif2features = {}\n",
    "text_vec_size = 2000\n",
    "for index, row in notif_df.iterrows():\n",
    "    notif2features[row.notif_id] = np.zeros([text_vec_size + 1 + 1 + 7])\n",
    "    if type(row.text) == str:\n",
    "        words = [int(word) for word in row.text.split(' ')]\n",
    "        notif2features[row.notif_id][words] = 1\n",
    "    \n",
    "    notif2features[row.notif_id][text_vec_size + 1] = category_prob[row.category]\n",
    "    \n",
    "    notif2features[row.notif_id][text_vec_size + 2] = row.hour / 24.0\n",
    "    \n",
    "    notif2features[row.notif_id][text_vec_size + 2 + row.day_of_week-1] = 1\n",
    "        \n",
    "notifs_in_category = notif_df[notif_df.category == CATEGORY].notif_id.unique()\n",
    "print(len(notifs_in_category))\n",
    "notif_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating user features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2589286it [02:15, 19170.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2589287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4009914it [11:43, 5698.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "user2features = {}\n",
    "categorical_features_dim = 0\n",
    "numerical_features_dim = 3\n",
    "\n",
    "FORCE_NEW = True\n",
    "\n",
    "if FORCE_NEW or (not path.exists('../data/users2num_features.pkl')):\n",
    "    print('creating user features...')\n",
    "    \n",
    "    users2rate = {}\n",
    "    tempdf = pd.read_csv('../data/user_rate.csv')\n",
    "    users2rate['else'] = tempdf.rate.median()\n",
    "    for index, row in tqdm(tempdf.iterrows()):\n",
    "        users2rate[row.user_id] = row.rate\n",
    "    print(len(users2rate))\n",
    "    tempdf = None\n",
    "    \n",
    "    users_df = pd.read_csv(DATA_PATH + 'users.csv')\n",
    "    \n",
    "    for index, row in tqdm(users_df.iterrows()):\n",
    "        user2features[row.user_id] = np.zeros([categorical_features_dim + numerical_features_dim + 1])\n",
    "        user2features[row.user_id][0:3] = np.array([row.N1, row.N2, row.N3])\n",
    "        user2features[row.user_id][3] = users2rate[row.user_id] if row.user_id in users2rate else users2rate['else']\n",
    "        \n",
    "    users_df = None\n",
    "    users2rate = None\n",
    "    with open('../data/users2num_features.pkl', 'wb') as f:\n",
    "        pkl.dump(file=f, obj=user2features)\n",
    "else:\n",
    "    print('loading user features...')\n",
    "    with open('../data/users2num_features.pkl','rb') as f:\n",
    "        user2features = pkl.load(f)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2009)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USERS_FEATURES = next(iter(user2features.values())).shape[0]\n",
    "NOTIFS_FEATURES = next(iter(notif2features.values())).shape[0]\n",
    "USERS_FEATURES, NOTIFS_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered = train_df[train_df.notif_id.isin(notifs_in_category)].copy()\n",
    "train_df_filtered.shape\n",
    "train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data, batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.data.shape[0] // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        Y = self.data[indexes,2]\n",
    "        _X = self.data[indexes]\n",
    "        X = np.zeros([_X.shape[0], USERS_FEATURES + NOTIFS_FEATURES])\n",
    "        X[:,0:USERS_FEATURES] = [user2features[user_id] for user_id in _X[:,0]]\n",
    "        X[:,USERS_FEATURES:USERS_FEATURES + NOTIFS_FEATURES] = [notif2features[notif_id] for notif_id in _X[:,1]]\n",
    "        weight = np.array([0.05 if y == 0 else 0.95 for y in Y])\n",
    "        return X, Y, weight\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.data.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (11662442, 3) test.shape: (614470, 3)\n"
     ]
    }
   ],
   "source": [
    "msk = np.random.rand(len(train_df_filtered)) < 0.95\n",
    "train = train_df_filtered[msk]\n",
    "test = train_df_filtered[~msk]\n",
    "print('train.shape:', train.shape, 'test.shape:', test.shape)\n",
    "train_batch_generator = DataGenerator(train.values, batch_size=TRAIN_BATCH_SIZE)\n",
    "valid_batch_generator = DataGenerator(test.values , batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1b19174bf4444ab154ba258b7124af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11662), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=614), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (2.2441435724501266, 0.05951791530944625, 0.37451019388660545)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of   3%|▎         | 20/614 [02:31<1:14:49,  7.56s/it]>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vahid/.virtualenvs/tensorflow/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 866, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/vahid/.virtualenvs/tensorflow/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 1085, in close\n",
      "    self._decr_instances(self)\n",
      "  File \"/Users/vahid/.virtualenvs/tensorflow/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 448, in _decr_instances\n",
      "    cls.monitor.exit()\n",
      "  File \"/Users/vahid/.virtualenvs/tensorflow/lib/python3.6/site-packages/tqdm/_monitor.py\", line 50, in exit\n",
      "    self.join()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/threading.py\", line 1053, in join\n",
      "    raise RuntimeError(\"cannot join current thread\")\n",
      "RuntimeError: cannot join current thread\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-21b0a707e269>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mxg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxg_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./models/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCATEGORY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_model.model'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/xgboost/python-package/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/apps/xgboost/python-package/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1060\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1061\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1062\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1063\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def run_test(model):\n",
    "    actuals = np.array([])\n",
    "    predicted = np.array([])\n",
    "    for batch_idx in tqdm(range(test.shape[0] // TRAIN_BATCH_SIZE), leave=False):\n",
    "        x, y, _ = valid_batch_generator[batch_idx]\n",
    "\n",
    "        xg_test = xgb.DMatrix(x, label=y)\n",
    "\n",
    "        actuals = np.concatenate([actuals, y])\n",
    "        predicted = np.concatenate([predicted, model.predict(xg_test)]) \n",
    "        \n",
    "    return weighted_logloss(actuals,predicted), actuals.mean(), predicted.mean()\n",
    "LAST_BATCH_DONE = 0\n",
    "\n",
    "def weighted_logloss(y,h):\n",
    "    return (-y * np.log(h) - 5*(1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "for batch_idx in tqdm(range(train.shape[0] // TRAIN_BATCH_SIZE)):\n",
    "    if batch_idx < LAST_BATCH_DONE:\n",
    "        continue\n",
    "    if batch_idx == 0:\n",
    "        model_name = None\n",
    "\n",
    "#     params = {'objective': 'reg:linear','eval_metric':'rmse', 'verbose': False}\n",
    "    params = {'objective': 'binary:logistic','eval_metric':'logloss', 'verbose': False}\n",
    "    param['gpu_id'] = 0\n",
    "    param['max_bin'] = 32\n",
    "    param['tree_method'] = 'gpu_hist'\n",
    "    data_batch = train_batch_generator[batch_idx]\n",
    "    x, y, _ = data_batch\n",
    "    \n",
    "    xg_train = xgb.DMatrix(x, label=y)\n",
    "    model = xgb.train(params, xg_train, 15, xgb_model=model_name)\n",
    "    \n",
    "    model_name = './models/'+str(CATEGORY)+'_model.model'\n",
    "    model.save_model(model_name)\n",
    "    if batch_idx % (1 + test.shape[0] // TRAIN_BATCH_SIZE) == 0:\n",
    "        print(batch_idx, run_test(model))\n",
    "    LAST_BATCH_DONE = batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[test_df.notif_id.isin(notifs_in_category)].shape[0] / test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['interaction'] = test_df.user_id.apply(lambda x: 0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 100000\n",
    "test_data_generator = DataGenerator(test_df.values, shuffle=False, batch_size=TEST_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = np.array([])\n",
    "for batch_idx in tqdm(range(1+(test_df.shape[0] // TEST_BATCH_SIZE))):\n",
    "    x = test_data_generator[batch_idx][0]\n",
    "    xg_test = xgb.DMatrix(x)\n",
    "\n",
    "    predicted = np.concatenate([predicted, model.predict(xg_test)]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert predicted.shape[0] == test_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted = np.concatenate([predicted, np.zeros(test_df.shape[0]-predicted.shape[0])])\n",
    "# predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['xg_preds_' + CATEGORY] = predicted\n",
    "predicted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_submission(filename, data):\n",
    "    out = ''\n",
    "    for item in tqdm(data):\n",
    "        out += \"%s\\n\" % np.round(item, 3)\n",
    "        \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creat_submission('../subs/xgboost_1st.txt', test_df['xg_preds_' + CATEGORY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../subs/only_user_rate_effect_squared.txt', 'r') as f:\n",
    "    squared_simple = f.read().split('\\n')\n",
    "squared_simple = squared_simple[:-1]\n",
    "len(squared_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a baseline\n",
    "squared_simple = [float(x) for x in squared_simple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['simple'] = squared_simple\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = np.zeros(test_df.shape[0])\n",
    "from tqdm import tqdm\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "    if row.notif_id in notifs_in_category:\n",
    "        submission[index] = row.xg_preds\n",
    "    else:\n",
    "        submission[index] = row.simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creat_submission('../subs/xgboost_small_on_cat7.txt', submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
