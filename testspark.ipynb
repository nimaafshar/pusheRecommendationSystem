{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nima\\Anaconda3\\lib\\site-packages\\ipykernel\\parentpoller.py:116: UserWarning: Parent poll failed.  If the frontend dies,\n",
      "                the kernel may be left running.  Please let us know\n",
      "                about your system (bitness, Python, etc.) at\n",
      "                ipython-dev@scipy.org\n",
      "  ipython-dev@scipy.org\"\"\")\n"
     ]
    }
   ],
   "source": [
    "# To find out where the pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-12DLPM0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "notifs.txt MapPartitionsRDD[65] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating words count\n",
    "text_file = sc.textFile('')\n",
    "text_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-c0afd69609c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'show'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"icons.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [f'F{i}' for i in range(1,201)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = df.groupby(l).agg(fn.count('notif_id')).alias('n_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[F1: string, F2: string, F3: string, F4: string, F5: string, F6: string, F7: string, F8: string, F9: string, F10: string, F11: string, F12: string, F13: string, F14: string, F15: string, F16: string, F17: string, F18: string, F19: string, F20: string, F21: string, F22: string, F23: string, F24: string, F25: string, F26: string, F27: string, F28: string, F29: string, F30: string, F31: string, F32: string, F33: string, F34: string, F35: string, F36: string, F37: string, F38: string, F39: string, F40: string, F41: string, F42: string, F43: string, F44: string, F45: string, F46: string, F47: string, F48: string, F49: string, F50: string, F51: string, F52: string, F53: string, F54: string, F55: string, F56: string, F57: string, F58: string, F59: string, F60: string, F61: string, F62: string, F63: string, F64: string, F65: string, F66: string, F67: string, F68: string, F69: string, F70: string, F71: string, F72: string, F73: string, F74: string, F75: string, F76: string, F77: string, F78: string, F79: string, F80: string, F81: string, F82: string, F83: string, F84: string, F85: string, F86: string, F87: string, F88: string, F89: string, F90: string, F91: string, F92: string, F93: string, F94: string, F95: string, F96: string, F97: string, F98: string, F99: string, F100: string, F101: string, F102: string, F103: string, F104: string, F105: string, F106: string, F107: string, F108: string, F109: string, F110: string, F111: string, F112: string, F113: string, F114: string, F115: string, F116: string, F117: string, F118: string, F119: string, F120: string, F121: string, F122: string, F123: string, F124: string, F125: string, F126: string, F127: string, F128: string, F129: string, F130: string, F131: string, F132: string, F133: string, F134: string, F135: string, F136: string, F137: string, F138: string, F139: string, F140: string, F141: string, F142: string, F143: string, F144: string, F145: string, F146: string, F147: string, F148: string, F149: string, F150: string, F151: string, F152: string, F153: string, F154: string, F155: string, F156: string, F157: string, F158: string, F159: string, F160: string, F161: string, F162: string, F163: string, F164: string, F165: string, F166: string, F167: string, F168: string, F169: string, F170: string, F171: string, F172: string, F173: string, F174: string, F175: string, F176: string, F177: string, F178: string, F179: string, F180: string, F181: string, F182: string, F183: string, F184: string, F185: string, F186: string, F187: string, F188: string, F189: string, F190: string, F191: string, F192: string, F193: string, F194: string, F195: string, F196: string, F197: string, F198: string, F199: string, F200: string, count(notif_id): bigint]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[F1: string, F2: string, F3: string, F4: string, F5: string, F6: string, F7: string, F8: string, F9: string, F10: string, F11: string, F12: string, F13: string, F14: string, F15: string, F16: string, F17: string, F18: string, F19: string, F20: string, F21: string, F22: string, F23: string, F24: string, F25: string, F26: string, F27: string, F28: string, F29: string, F30: string, F31: string, F32: string, F33: string, F34: string, F35: string, F36: string, F37: string, F38: string, F39: string, F40: string, F41: string, F42: string, F43: string, F44: string, F45: string, F46: string, F47: string, F48: string, F49: string, F50: string, F51: string, F52: string, F53: string, F54: string, F55: string, F56: string, F57: string, F58: string, F59: string, F60: string, F61: string, F62: string, F63: string, F64: string, F65: string, F66: string, F67: string, F68: string, F69: string, F70: string, F71: string, F72: string, F73: string, F74: string, F75: string, F76: string, F77: string, F78: string, F79: string, F80: string, F81: string, F82: string, F83: string, F84: string, F85: string, F86: string, F87: string, F88: string, F89: string, F90: string, F91: string, F92: string, F93: string, F94: string, F95: string, F96: string, F97: string, F98: string, F99: string, F100: string, F101: string, F102: string, F103: string, F104: string, F105: string, F106: string, F107: string, F108: string, F109: string, F110: string, F111: string, F112: string, F113: string, F114: string, F115: string, F116: string, F117: string, F118: string, F119: string, F120: string, F121: string, F122: string, F123: string, F124: string, F125: string, F126: string, F127: string, F128: string, F129: string, F130: string, F131: string, F132: string, F133: string, F134: string, F135: string, F136: string, F137: string, F138: string, F139: string, F140: string, F141: string, F142: string, F143: string, F144: string, F145: string, F146: string, F147: string, F148: string, F149: string, F150: string, F151: string, F152: string, F153: string, F154: string, F155: string, F156: string, F157: string, F158: string, F159: string, F160: string, F161: string, F162: string, F163: string, F164: string, F165: string, F166: string, F167: string, F168: string, F169: string, F170: string, F171: string, F172: string, F173: string, F174: string, F175: string, F176: string, F177: string, F178: string, F179: string, F180: string, F181: string, F182: string, F183: string, F184: string, F185: string, F186: string, F187: string, F188: string, F189: string, F190: string, F191: string, F192: string, F193: string, F194: string, F195: string, F196: string, F197: string, F198: string, F199: string, F200: string, count(notif_id): bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1414.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#6267L])\n   +- InMemoryTableScan\n         +- InMemoryRelation [F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n                  +- Exchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n                     +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n                        +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2831)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2830)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n+- Exchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n   +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n      +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:89)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:59)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:276)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:105)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:104)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:310)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 37 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n+- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n   +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:102)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 69 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n+- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 79 more\r\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\r\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\r\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\r\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\r\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\r\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:102)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 90 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-8bc332962900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m         \"\"\"\n\u001b[1;32m--> 522\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-2.4.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1414.count.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange SinglePartition\n+- *(1) HashAggregate(keys=[], functions=[partial_count(1)], output=[count#6267L])\n   +- InMemoryTableScan\n         +- InMemoryRelation [F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields], StorageLevel(disk, memory, deserialized, 1 replicas)\n               +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n                  +- Exchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n                     +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n                        +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:296)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2831)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)\r\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3365)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3364)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2830)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n+- Exchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n   +- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n      +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.buildBuffers(InMemoryRelation.scala:89)\r\n\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder.cachedColumnBuffers(InMemoryRelation.scala:59)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.filteredCachedBatches(InMemoryTableScanExec.scala:276)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD$lzycompute(InMemoryTableScanExec.scala:105)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.inputRDD(InMemoryTableScanExec.scala:104)\r\n\tat org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.doExecute(InMemoryTableScanExec.scala:310)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:374)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:151)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:610)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 37 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields)\n+- HashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n   +- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:102)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 69 more\r\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nHashAggregate(keys=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 176 more fields], functions=[partial_count(notif_id#10)], output=[F1#11, F2#12, F3#13, F4#14, F5#15, F6#16, F7#17, F8#18, F9#19, F10#20, F11#21, F12#22, F13#23, F14#24, F15#25, F16#26, F17#27, F18#28, F19#29, F20#30, F21#31, F22#32, F23#33, F24#34, ... 177 more fields])\n+- FileScan csv [notif_id#10,F1#11,F2#12,F3#13,F4#14,F5#15,F6#16,F7#17,F8#18,F9#19,F10#20,F11#21,F12#22,F13#23,F14#24,F15#25,F16#26,F17#27,F18#28,F19#29,F20#30,F21#31,F22#32,F23#33,... 177 more fields] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/D:/recommender/icons.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<notif_id:string,F1:string,F2:string,F3:string,F4:string,F5:string,F6:string,F7:string,F8:s...\n\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.doExecute(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)\r\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 79 more\r\nCaused by: java.lang.IllegalArgumentException: Unsupported class file major version 55\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)\r\n\tat org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)\r\n\tat org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:49)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:517)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3$$anonfun$visitMethodInsn$2.apply(ClosureCleaner.scala:500)\r\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)\r\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\r\n\tat scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\r\n\tat org.apache.spark.util.FieldAccessFinder$$anon$3.visitMethodInsn(ClosureCleaner.scala:500)\r\n\tat org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)\r\n\tat org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)\r\n\tat org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:307)\r\n\tat org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:306)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:306)\r\n\tat org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)\r\n\tat org.apache.spark.SparkContext.clean(SparkContext.scala:2326)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:850)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(RDD.scala:849)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\r\n\tat org.apache.spark.rdd.RDD.mapPartitionsWithIndex(RDD.scala:849)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:102)\r\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1.apply(HashAggregateExec.scala:95)\r\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\r\n\t... 90 more\r\n"
     ]
    }
   ],
   "source": [
    "gr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
